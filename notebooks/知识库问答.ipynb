{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdffbc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be365456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 0. 配置常量 ------------------\n",
    "OLLAMA_MODEL = \"qwen2.5\"\n",
    "EMBED_MODEL = \"bge-m3\"\n",
    "URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "THREAD_ID = \"def234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d8f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 1. 初始化模型与向量库 ------------------\n",
    "llm = ChatOllama(model=OLLAMA_MODEL, temperature=0)\n",
    "embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c099af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 2. 文档抓取与切分 ------------------\n",
    "def load_and_index(url: str) -> InMemoryVectorStore:\n",
    "    \"\"\"\n",
    "    抓取网页 → 按指定规则切分 → 向量化 → 返回已填充的向量库\n",
    "    \"\"\"\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": bs4.SoupStrainer(\n",
    "                class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "    )\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    vector_store.add_documents(splits)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store = load_and_index(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f289d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 3. 检索工具 ------------------\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    根据用户问题，从已索引的文章中检索最相关片段。\n",
    "    返回 (拼接后的文本, 原始文档列表)，便于调试\n",
    "    \"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"来源：{doc.metadata}\\n内容：{doc.page_content}\" for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf8498be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 4. 构建 ReAct Agent ------------------\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2340ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 5. 统一对话入口 ------------------\n",
    "def chat(questions: List[str], thread_id: str = THREAD_ID) -> None:\n",
    "    \"\"\"\n",
    "    批量向 Agent 提问并流式输出回答\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    for q in questions:\n",
    "        for event in agent_executor.stream(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": q}]},\n",
    "            stream_mode=\"values\",\n",
    "            config=config,\n",
    "        ):\n",
    "            event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69cad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "任务分解（Task Decomposition）的标准方法是什么？找到答案后，再帮我查阅该方法的常见扩展或改进方案。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "根据检索到的信息，任务分解（Task Decomposition）的标准方法主要包括以下几种：\n",
      "\n",
      "1. **通过简单的提示进行LLM（大型语言模型）任务分解**：例如，使用类似于“步骤为XYZ。\\n1.”或“What are the subgoals for achieving XYZ?”这样的简单提示来引导LLM。\n",
      "\n",
      "2. **使用特定任务的指令**：比如写小说时可以给出“编写故事大纲”的指令。\n",
      "\n",
      "3. **通过人类输入进行任务分解**。\n",
      "\n",
      "这些方法提供了任务分解的不同策略和视角。接下来，我将查找这些方法的一些常见扩展或改进方案。\n",
      "Tool Calls:\n",
      "  retrieve (4a85ffc3-d150-4221-8ffd-e3ce1a42f8d6)\n",
      " Call ID: 4a85ffc3-d150-4221-8ffd-e3ce1a42f8d6\n",
      "  Args:\n",
      "    query: task decomposition common extensions or improvements\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "来源：{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "内容：Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "来源：{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "内容：Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "任务分解（Task Decomposition）的标准方法主要包括以下几种：\n",
      "\n",
      "1. **通过简单的提示进行LLM任务分解**：例如，使用类似于“步骤为XYZ。\\n1.”或“What are the subgoals for achieving XYZ?”这样的简单提示来引导LLM。\n",
      "2. **使用特定任务的指令**：比如写小说时可以给出“编写故事大纲”的指令。\n",
      "3. **通过人类输入进行任务分解**。\n",
      "\n",
      "这些方法提供了任务分解的不同策略和视角。接下来，我将查找这些方法的一些常见扩展或改进方案。\n",
      "Tool Calls:\n",
      "  retrieve (2273c701-e697-4b20-8912-35bb7be62971)\n",
      " Call ID: 2273c701-e697-4b20-8912-35bb7be62971\n",
      "  Args:\n",
      "    query: task decomposition common extensions and improvements\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "来源：{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "内容：Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "来源：{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "内容：Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "任务分解（Task Decomposition）的标准方法主要包括以下几种：\n",
      "\n",
      "1. **通过简单的提示进行LLM任务分解**：例如，使用类似于“步骤为XYZ。\\n1.”或“What are the subgoals for achieving XYZ?”这样的简单提示来引导LLM。\n",
      "2. **使用特定任务的指令**：比如写小说时可以给出“编写故事大纲”的指令。\n",
      "3. **通过人类输入进行任务分解**。\n",
      "\n",
      "这些方法提供了任务分解的不同策略和视角。接下来，我将查找这些方法的一些常见扩展或改进方案。\n",
      "\n",
      "### 常见扩展或改进方案\n",
      "\n",
      "1. **链式思考（Chain of thought, CoT）的扩展**：\n",
      "   - **树状思考（Tree of Thoughts, ToT）**：在每个步骤上探索多种推理可能性，生成多个想法，并创建一个树形结构。ToT可以采用广度优先搜索（BFS）或深度优先搜索（DFS），每种状态通过提示进行分类评估或多数投票。\n",
      "\n",
      "2. **LLM+P方法的改进**：\n",
      "   - 优化PDDL的定义和规划器的选择，以提高规划效率和准确性。\n",
      "   - 结合自动化的辅助工具，减少人工输入的工作量。\n",
      "\n",
      "3. **人类输入任务分解的改进**：\n",
      "   - 结合自动化工具，提供更智能的任务分解建议，减轻人类负担。\n",
      "   - 使用机器学习技术预测最佳任务分解方案，提高效率。\n",
      "\n",
      "这些扩展或改进方案旨在进一步提升任务分解的效果和实用性。例如，ToT通过探索多种推理路径可以提供更全面的任务解决方案；而PDDL+规划器的方法则可以通过优化来提高长期复杂任务的处理能力。\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"任务分解（Task Decomposition）的标准方法是什么？找到答案后，再帮我查阅该方法的常见扩展或改进方案。\"\n",
    "]\n",
    "chat(questions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
