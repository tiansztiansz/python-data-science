{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3156e48",
   "metadata": {},
   "source": [
    "### ollama本地部署大模型\n",
    "\n",
    "- 首先前往[官网](https://ollama.com/)下载软件并安装\n",
    "  \n",
    "- 在windows系统搜索框搜索`终端`并打开\n",
    "  \n",
    "- 接着可以在[模型列表](https://ollama.com/search)中找想要的下载的模型，复制下载命令即可，例如我下载qwen2.5-7b的模型：`ollama pull qwen2.5`，这里会默认下载7b的。若下载过程中卡住，则用键盘点击 ctrl + c 终止下载，然后再运行下载命令即可\n",
    "  \n",
    "- 你还可以测试模型是否能正常运行：`ollama run qwen2.5`\n",
    "- 后续运行大模型时，需要确保ollama运用程序已开启，可以查看windows右下角的隐藏图标。若未看到ollama图标，可以在搜索框搜索ollama并点击运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ac7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4b69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 初始化模型 ------------------\n",
    "llm = ChatOllama(model=\"qwen2.5\", temperature=0)  # temperature=0 保证输出稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed34dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ 通用调用封装 ------------------\n",
    "def run_chain(history: List[BaseMessage]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    统一调用 LLM 并返回结构化结果\n",
    "    :param history: 包含 system/user/ai 的历史消息\n",
    "    :return: LLM 返回的 JSON 字符串解析后的 dict\n",
    "    \"\"\"\n",
    "    response = llm.invoke(history)\n",
    "    response.pretty_print()  \n",
    "    try:\n",
    "        return json.loads(response.content)  # 解析 JSON，方便下游使用\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"raw\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8439558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【示例 1：情绪抽取】\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\"emotion\": \"惊讶\"}\n",
      "解析结果： {'emotion': '惊讶'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------ 示例 1：情绪抽取 ------------------\n",
    "emotion_schema = {\"emotion\": \"开心 | 生气 | 悲伤 | 惊讶 | 恐惧 | 厌恶 | 中性\"}\n",
    "emotion_examples = [\n",
    "    (\"今天发工资啦，晚饭吃火锅庆祝！\", \"开心\"),\n",
    "    (\"老板又临时加需求，真受不了！\", \"生气\"),\n",
    "    (\"我的狗走丢了，已经三天没回家。\", \"悲伤\"),\n",
    "]\n",
    "\n",
    "\n",
    "def build_emotion_prompt(target: str) -> List[BaseMessage]:\n",
    "    \"\"\"构造情绪抽取的 few-shot prompt\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"请从句子中提取用户的情绪（emotion），并以 JSON 形式返回，例如：\\n\"\n",
    "            f\"{emotion_schema}\"\n",
    "        )\n",
    "    ]\n",
    "    for sent, label in emotion_examples:\n",
    "        messages.extend([HumanMessage(sent), AIMessage(f'{{\"emotion\": \"{label}\"}}')])\n",
    "    messages.append(HumanMessage(target))\n",
    "    return messages\n",
    "\n",
    "\n",
    "print(\"【示例 1：情绪抽取】\")\n",
    "emotion_result = run_chain(build_emotion_prompt(\"没想到电影结局这么反转，完全没猜到！\"))\n",
    "print(\"解析结果：\", emotion_result, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ffad926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【示例 2：人物信息抽取】\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "{\"name\": \"张小明\", \"hair_color\": \"黑色\", \"height_cm\": 172}\n",
      "解析结果： {'name': '张小明', 'hair_color': '黑色', 'height_cm': 172}\n"
     ]
    }
   ],
   "source": [
    "# ------------------ 示例 2：人物信息抽取 ------------------\n",
    "person_schema = {\"name\": \"姓名\", \"hair_color\": \"发色\", \"height_cm\": \"身高(cm)\"}\n",
    "person_examples = [\n",
    "    (\n",
    "        \"李雷身高180厘米，头发是棕色的。\",\n",
    "        {\"name\": \"李雷\", \"hair_color\": \"棕色\", \"height_cm\": 180},\n",
    "    ),\n",
    "    (\n",
    "        \"金发的韩梅梅只有158cm高。\",\n",
    "        {\"name\": \"韩梅梅\", \"hair_color\": \"金色\", \"height_cm\": 158},\n",
    "    ),\n",
    "    (\n",
    "        \"王芳染了一头红发，身高约165厘米。\",\n",
    "        {\"name\": \"王芳\", \"hair_color\": \"红色\", \"height_cm\": 165},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def build_person_prompt(target: str) -> List[BaseMessage]:\n",
    "    \"\"\"构造人物信息抽取的 few-shot prompt\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            \"请从句子中提取人物姓名、发色（hair_color）和身高（height_cm），\"\n",
    "            \"并以 JSON 形式返回，例如：\\n\"\n",
    "            f\"{person_schema}\"\n",
    "        )\n",
    "    ]\n",
    "    for sent, info in person_examples:\n",
    "        messages.extend(\n",
    "            [\n",
    "                HumanMessage(sent),\n",
    "                AIMessage(str(info).replace(\"'\", '\"')),  # 转双引号\n",
    "            ]\n",
    "        )\n",
    "    messages.append(HumanMessage(target))\n",
    "    return messages\n",
    "\n",
    "\n",
    "print(\"【示例 2：人物信息抽取】\")\n",
    "person_result = run_chain(\n",
    "    build_person_prompt(\"张小明是黑发，据说他现在已经长到172厘米了。\")\n",
    ")\n",
    "print(\"解析结果：\", person_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
