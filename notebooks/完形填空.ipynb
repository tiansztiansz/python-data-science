{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7770a046",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a461f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89aa4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4  # 模型学习训练集的轮次\n",
    "BATCH_SIZE = 4  # 模型并行学习的样本数量\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"../models/chinese-lert-base\"  # 预训练模型路径\n",
    "CSV_PATH = \"../datasets/语言建模数据集.csv\"  # 只有文本的无监督数据集\n",
    "OUPUT_CHECKPOINT_PATH = \"../models/fill_mask_checkpoint\"  # 训练过程中产生的模型文件\n",
    "OUTPUT_MODEL_PATH = \"../models/fill_mask_model\"  # 微调后的文本分类模型路径\n",
    "VAL_SIZE = 0.1  # 验证集的占比\n",
    "MAX_LENGTH = 256  # 输入文本的token长度，lert模型最大只能是512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1b707a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7e7a691bea43caa1f559cb5ea2b42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34374bffdaf94a2f8be69a7cd4485234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=CSV_PATH)\n",
    "eli5 = dataset[\"train\"].train_test_split(test_size=VAL_SIZE)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    max_length=MAX_LENGTH,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], max_length=MAX_LENGTH, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "lm_dataset = eli5.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8c514",
   "metadata": {},
   "source": [
    "#### 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31042abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/chinese-lert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 06:49, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.453130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.413619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.544467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.427079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/fill_mask_model\\\\tokenizer_config.json',\n",
       " '../models/fill_mask_model\\\\special_tokens_map.json',\n",
       " '../models/fill_mask_model\\\\vocab.txt',\n",
       " '../models/fill_mask_model\\\\added_tokens.json',\n",
       " '../models/fill_mask_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "\n",
    "# 使得模型的tensor连续，避免报错\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUPUT_CHECKPOINT_PATH,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.half()\n",
    "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8624da5",
   "metadata": {},
   "source": [
    "#### 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f79363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.1729099601507187, 'token': 1916, 'token_str': '够', 'sequence': '一 般 般 ， 盖 子 不 够 。'}, {'score': 0.12599192559719086, 'token': 1962, 'token_str': '好', 'sequence': '一 般 般 ， 盖 子 不 好 。'}, {'score': 0.10024310648441315, 'token': 1920, 'token_str': '大', 'sequence': '一 般 般 ， 盖 子 不 大 。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=OUTPUT_MODEL_PATH\n",
    ")\n",
    "\n",
    "text = \"一般般，盖子不[MASK]。\"\n",
    "mask_result = mask_filler(inputs=text, top_k=3)\n",
    "print(mask_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
