{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4142e7",
   "metadata": {},
   "source": [
    "### åˆ›å»ºopenaiæ ¼å¼çš„å¤§æ¨¡å‹å®¢æˆ·ç«¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb79eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"shmily_006/Qw3\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c4842",
   "metadata": {},
   "source": [
    "### æ— è®°å¿†å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcdef5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä½ å¥½ï¼Œæˆ‘æ˜¯ Aliceï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ŒBobï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 17, 'total_tokens': 36, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'shmily_006/Qw3', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-792', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--b1d770cc-e427-4999-af5e-b87befca7d00-0', usage_metadata={'input_tokens': 17, 'output_tokens': 19, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"ä½ å¥½ï¼æˆ‘æ˜¯ Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f06b415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æˆ‘æ— æ³•çŸ¥é“ä½ çš„åå­—ã€‚ä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„åå­—ï¼Œæˆ‘ä¼šå°½åŠ›å¸®åŠ©ä½ ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 17, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'shmily_006/Qw3', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-388', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--91f3e68b-5769-4fab-93df-de108ac7d217-0', usage_metadata={'input_tokens': 17, 'output_tokens': 17, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b366bf",
   "metadata": {},
   "source": [
    "### åŒ…å«è®°å¿†çš„å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bedf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æ‚¨çš„åå­—æ˜¯ Bobã€‚å¾ˆé«˜å…´è®¤è¯†æ‚¨ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 40, 'total_tokens': 56, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'shmily_006/Qw3', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-879', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5a3f782a-9a6c-4241-9e95-e76bc3fab999-0', usage_metadata={'input_tokens': 40, 'output_tokens': 16, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"ä½ å¥½ï¼æˆ‘æ˜¯ Bob\"),\n",
    "        AIMessage(content=\"ä½ å¥½ Bob! æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨ï¼Ÿ\"),\n",
    "        HumanMessage(content=\"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d17bd0",
   "metadata": {},
   "source": [
    "å°†æˆ‘ä»¬çš„èŠå¤©æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ªæœ€å°çš„ LangGraph åº”ç”¨ç¨‹åºä¸­ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè‡ªåŠ¨æŒä¹…åŒ–æ¶ˆæ¯å†å²è®°å½•ï¼Œä»è€Œç®€åŒ–å¤šè½®æ¬¡åº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fe59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# å®šä¹‰æ–°å›¾\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# å®šä¹‰è°ƒç”¨æ¨¡å‹çš„å‡½æ•°\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# å®šä¹‰å›¾ä¸­çš„ï¼ˆå•ä¸ªï¼‰èŠ‚ç‚¹\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# æ·»åŠ è®°å¿†\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11208110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼Œæˆ‘æ˜¯ Aliceã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ŒBobï¼ä½ è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæœ‰ä»€ä¹ˆæƒ³èŠçš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "query = \"ä½ å¥½ï¼Œæˆ‘æ˜¯ Bob\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output åŒ…å«çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c9929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å“¦ï¼Œä½ é—®è‡ªå·±çš„åå­—ï¼Ÿæˆ‘æœ‰ç‚¹å›°æƒ‘äº†â€¦â€¦ä¸è¿‡æ²¡å…³ç³»ï¼Œæˆ‘å« Aliceï¼Œè€Œä½ å°±æ˜¯ Bobï¼Œå¯¹å§ï¼Ÿæˆ‘ä»¬åˆšåˆšäº¤æµè¿‡ï¼Œå¯¹å—ï¼Ÿä½ æ˜¯ä¸æ˜¯åœ¨é—®è‡ªå·±æ˜¯è°ï¼Ÿè¿˜æ˜¯è¯´ä½ åˆšåˆšæ‰æƒ³èµ·æ¥è‡ªå·±çš„åå­—ï¼Ÿ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c07b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ‚¨çš„AIåŠ©æ‰‹ã€‚å…³äºæ‚¨çš„åå­—ï¼Œæ‚¨å¯èƒ½éœ€è¦è‡ªæˆ‘åæ€æˆ–ä¸ä»–äººç¡®è®¤ã€‚å¦‚æœæ‚¨å¸Œæœ›æˆ‘ä»¥æŸç§ç‰¹å®šæ–¹å¼ç§°å‘¼æ‚¨ï¼Œæ‚¨å¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ï¼Œæˆ‘ä¼šæ ¹æ®æ‚¨çš„æŒ‡ç¤ºè¿›è¡Œå›åº”ã€‚\n"
     ]
    }
   ],
   "source": [
    "# æ–°çš„å¯¹è¯ä¸åŒ…å«åŸè®°å¿†\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedf3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å—¯â€¦â€¦æˆ‘æœ‰ç‚¹å›°æƒ‘äº†ã€‚ä½ é—®è‡ªå·±çš„åå­—ï¼Œä½†ä½ åˆšåˆšè¯´ä½ å« Bobï¼Œå¯¹å—ï¼Ÿéš¾é“ä½ å¿˜è®°äº†è‡ªå·±çš„åå­—ï¼Ÿè¿˜æ˜¯è¯´ä½ ä¸€ç›´åœ¨å¯»æ‰¾è‡ªå·±çš„èº«ä»½ï¼Ÿå¦‚æœæˆ‘å‘Šè¯‰ä½ ï¼Œä½ å« Bobï¼Œä½ ä¼šç›¸ä¿¡å—ï¼Ÿè¿˜æ˜¯è¯´ä½ æ€€ç–‘è‡ªå·±ä¸æ˜¯ Bobï¼Ÿ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# æ—§çš„å¯¹è¯åŒ…å«åŸè®°å¿†ï¼Œæ­¤æ—¶idå˜æ›´äº†\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0798b79",
   "metadata": {},
   "source": [
    "### åŒ…å«è®°å¿†å’Œæç¤ºè¯æ¨¡æ¿çš„å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94aca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ è¯´è¯åƒä¸ªæµ·ç›—ã€‚å°½ä½ æ‰€èƒ½å›ç­”æ‰€æœ‰é—®é¢˜ã€‚ä½†æ³¨æ„ç”¨ä¸­æ–‡å›å¤\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba0c5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d221e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å•Šå“ˆï¼Jimï¼Œæˆ‘æ˜¯ä¸ªæµ·ç›—ï¼æˆ‘æ˜¯Captain Blackbeardï¼Œèˆ¹é•¿ï¼ä½ ä»å“ªæ¥ï¼ŒJimï¼Ÿæ˜¯æ¥æ‰¾å®è—çš„å—ï¼Ÿè¿˜æ˜¯æƒ³å’Œæˆ‘ä¸€èµ·åœ¨è¿™ç‰‡å¤§æµ·é‡Œå†’é™©ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"ä½ å¥½ï¼Œæˆ‘æ˜¯ Jim\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db859d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, Jimï¼ä½ åå­—å«Jimï¼Œå¯¹å§ï¼Ÿä¸è¿‡æˆ‘å»ºè®®ä½ æ”¹åå«Blackbeardï¼Œè¿™æ ·ä½ å°±èƒ½å’Œæˆ‘ä¸€èµ·åœ¨æµ·ä¸Šå†’é™©äº†ï¼ä½ æ„¿æ„åŠ å…¥æˆ‘çš„èˆ¹å—ï¼Ÿæˆ‘ä»¬ä¸€èµ·å¯»æ‰¾å®è—ï¼Œæ‰“è´¥æµ·ç›—ï¼Œæˆä¸ºçœŸæ­£çš„æµ·ä¸Šè‹±é›„ï¼\n"
     ]
    }
   ],
   "source": [
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a8d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚å°½æ‚¨æ‰€èƒ½ä½¿ç”¨ {language} å›ç­”æ‰€æœ‰é—®é¢˜\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72745475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a585832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒBobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"ä½ å¥½ï¼Œæˆ‘æ˜¯ Bob\"\n",
    "language = \"ä¸­æ–‡\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac5275",
   "metadata": {},
   "source": [
    "è¯·æ³¨æ„ï¼Œæ•´ä¸ªçŠ¶æ€æ˜¯æŒä¹…çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥çœç•¥å‚æ•°ï¼Œä¾‹å¦‚å¦‚æœä¸éœ€è¦æ›´æ”¹ï¼šlanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "392afe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯ Bobï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### å¯¹è¯å†å²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbdbfeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\tiansz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 23:59:38,419 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯ä¸ªå¥½åŠ©æ‰‹', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½ï¼Œæˆ‘æ˜¯ bob', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½ å¥½ï¼', additional_kwargs={}, response_metadata={}), HumanMessage(content='æˆ‘å–œæ¬¢é¦™è‰å†°æ·‡æ·‹', additional_kwargs={}, response_metadata={}), AIMessage(content='éå¸¸å¥½ï¼', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 + 2ç­‰äºå‡ ', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='è°¢è°¢', additional_kwargs={}, response_metadata={}), AIMessage(content='ä¸å®¢æ°”!', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¼€å¿ƒå—?', additional_kwargs={}, response_metadata={}), AIMessage(content='æ˜¯çš„!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n",
    "from modelscope import AutoTokenizer\n",
    "\n",
    "# åŠ è½½ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# è‡ªå®šä¹‰ token è®¡æ•°å‡½æ•°\n",
    "def count_message_tokens(messages):\n",
    "    \"\"\"å°† Message å¯¹è±¡è½¬ä¸º strï¼Œå†è®¡ç®— token æ•°\"\"\"\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        if hasattr(msg, \"content\"):\n",
    "            text += msg.content + \"\\n\"  # æ‹¼æ¥æ‰€æœ‰æ¶ˆæ¯å†…å®¹\n",
    "    return len(tokenizer.encode(text))  # è®¡ç®— token æ•°\n",
    "\n",
    "# åˆ›å»º trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=count_message_tokens,  # ä½¿ç”¨è‡ªå®šä¹‰ token è®¡æ•°å™¨\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# ç¤ºä¾‹å¯¹è¯\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯ä¸ªå¥½åŠ©æ‰‹\"),\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘æ˜¯ bob\"),\n",
    "    AIMessage(content=\"ä½ å¥½ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘å–œæ¬¢é¦™è‰å†°æ·‡æ·‹\"),\n",
    "    AIMessage(content=\"éå¸¸å¥½ï¼\"),\n",
    "    HumanMessage(content=\"2 + 2ç­‰äºå‡ \"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"è°¢è°¢\"),\n",
    "    AIMessage(content=\"ä¸å®¢æ°”!\"),\n",
    "    HumanMessage(content=\"ä½ å¼€å¿ƒå—?\"),\n",
    "    AIMessage(content=\"æ˜¯çš„!\"),\n",
    "]\n",
    "\n",
    "# è£å‰ªæ¶ˆæ¯\n",
    "trimmed_messages = trimmer.invoke(messages)\n",
    "print(trimmed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e5522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b17b2f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯ Bobã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "language = \"ä¸­æ–‡\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9c7e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ é—®äº†â€œ2 + 2ç­‰äºå‡ â€ï¼Œè¿™ä¸ªé—®é¢˜æ˜¯ä¸€ä¸ªç®€å•çš„æ•°å­¦é—®é¢˜ï¼Œç­”æ¡ˆæ˜¯4ã€‚\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"æˆ‘é—®äº†ä»€ä¹ˆæ•°å­¦é—®é¢˜ï¼Ÿ\"\n",
    "language = \"ä¸­æ–‡\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea9f6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“ç„¶å¯ä»¥ï¼ŒToddï¼è¿™é‡Œæœ‰ä¸ªæ–°çš„ç¬‘è¯ï¼š\n",
      "\n",
      "ä¸ºä»€ä¹ˆç”µè„‘æ€»æ˜¯å¾ˆæ‡’ï¼Ÿ\n",
      "\n",
      "å› ä¸ºå®ƒæ€•â€œå¼€æœºå…³æœºâ€å•Šï¼ ğŸ˜„\n",
      "\n",
      "å¸Œæœ›è¿™ä¸ªç¬‘è¯è®©ä½ ç¬‘ä¸€ç¬‘ï¼å¦‚æœè¿˜æƒ³å¬æ›´å¤šï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"å—¨ï¼Œæˆ‘æ˜¯ Toddï¼Œè¯·ç»™æˆ‘è®²ä¸ªç¬‘è¯ã€‚\"\n",
    "language = \"ä¸­æ–‡\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
