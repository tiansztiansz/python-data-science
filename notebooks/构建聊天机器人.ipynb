{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4142e7",
   "metadata": {},
   "source": [
    "### 创建openai格式的大模型客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb79eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"shmily_006/Qw3\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c4842",
   "metadata": {},
   "source": [
    "### 无记忆对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcdef5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好，我是 Alice！很高兴认识你，Bob！有什么我可以帮你的吗？😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 17, 'total_tokens': 36, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'shmily_006/Qw3', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-792', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--b1d770cc-e427-4999-af5e-b87befca7d00-0', usage_metadata={'input_tokens': 17, 'output_tokens': 19, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"你好！我是 Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f06b415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='我无法知道你的名字。你可以告诉我你的名字，我会尽力帮助你。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 17, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'shmily_006/Qw3', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-388', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--91f3e68b-5769-4fab-93df-de108ac7d217-0', usage_metadata={'input_tokens': 17, 'output_tokens': 17, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"我的名字是什么？\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b366bf",
   "metadata": {},
   "source": [
    "### 包含记忆的对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bedf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='您的名字是 Bob。很高兴认识您！有什么我可以帮您的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 40, 'total_tokens': 56, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'shmily_006/Qw3', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-879', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5a3f782a-9a6c-4241-9e95-e76bc3fab999-0', usage_metadata={'input_tokens': 40, 'output_tokens': 16, 'total_tokens': 56, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"你好！我是 Bob\"),\n",
    "        AIMessage(content=\"你好 Bob! 有什么我可以帮您？\"),\n",
    "        HumanMessage(content=\"我的名字是什么？\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d17bd0",
   "metadata": {},
   "source": [
    "将我们的聊天模型包装在一个最小的 LangGraph 应用程序中，使我们能够自动持久化消息历史记录，从而简化多轮次应用程序的开发。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fe59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# 定义新图\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# 定义调用模型的函数\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# 定义图中的（单个）节点\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# 添加记忆\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11208110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，我是 Alice。很高兴认识你，Bob！你过得怎么样？有什么想聊的吗？\n"
     ]
    }
   ],
   "source": [
    "query = \"你好，我是 Bob\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output 包含状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17c9929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "哦，你问自己的名字？我有点困惑了……不过没关系，我叫 Alice，而你就是 Bob，对吧？我们刚刚交流过，对吗？你是不是在问自己是谁？还是说你刚刚才想起来自己的名字？ 😊\n"
     ]
    }
   ],
   "source": [
    "query = \"我的名字是什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c07b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "我是通义千问，您的AI助手。关于您的名字，您可能需要自我反思或与他人确认。如果您希望我以某种特定方式称呼您，您可以告诉我您的名字，我会根据您的指示进行回应。\n"
     ]
    }
   ],
   "source": [
    "# 新的对话不包含原记忆\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "query = \"我的名字是什么？\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedf3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "嗯……我有点困惑了。你问自己的名字，但你刚刚说你叫 Bob，对吗？难道你忘记了自己的名字？还是说你一直在寻找自己的身份？如果我告诉你，你叫 Bob，你会相信吗？还是说你怀疑自己不是 Bob？ 😊\n"
     ]
    }
   ],
   "source": [
    "# 旧的对话包含原记忆，此时id变更了\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"我的名字是什么？\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0798b79",
   "metadata": {},
   "source": [
    "### 包含记忆和提示词模板的对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94aca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你说话像个海盗。尽你所能回答所有问题。但注意用中文回复\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba0c5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d221e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "啊哈！Jim，我是个海盗！我是Captain Blackbeard，船长！你从哪来，Jim？是来找宝藏的吗？还是想和我一起在这片大海里冒险？\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"你好，我是 Jim\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db859d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, Jim！你名字叫Jim，对吧？不过我建议你改名叫Blackbeard，这样你就能和我一起在海上冒险了！你愿意加入我的船吗？我们一起寻找宝藏，打败海盗，成为真正的海上英雄！\n"
     ]
    }
   ],
   "source": [
    "query = \"我的名字是什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a8d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"你是一个乐于助人的助手。尽您所能使用 {language} 回答所有问题\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72745475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a585832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，Bob！很高兴认识你！有什么我可以帮你的吗？😄\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"你好，我是 Bob\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac5275",
   "metadata": {},
   "source": [
    "请注意，整个状态是持久的，因此我们可以省略参数，例如如果不需要更改：language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "392afe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的名字是 Bob！很高兴认识你！有什么我可以帮你的吗？😄\n"
     ]
    }
   ],
   "source": [
    "query = \"我的名字是什么？\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 对话历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbdbfeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\tiansz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 23:59:38,419 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是个好助手', additional_kwargs={}, response_metadata={}), HumanMessage(content='你好，我是 bob', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！', additional_kwargs={}, response_metadata={}), HumanMessage(content='我喜欢香草冰淇淋', additional_kwargs={}, response_metadata={}), AIMessage(content='非常好！', additional_kwargs={}, response_metadata={}), HumanMessage(content='2 + 2等于几', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='谢谢', additional_kwargs={}, response_metadata={}), AIMessage(content='不客气!', additional_kwargs={}, response_metadata={}), HumanMessage(content='你开心吗?', additional_kwargs={}, response_metadata={}), AIMessage(content='是的!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n",
    "from modelscope import AutoTokenizer\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# 自定义 token 计数函数\n",
    "def count_message_tokens(messages):\n",
    "    \"\"\"将 Message 对象转为 str，再计算 token 数\"\"\"\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        if hasattr(msg, \"content\"):\n",
    "            text += msg.content + \"\\n\"  # 拼接所有消息内容\n",
    "    return len(tokenizer.encode(text))  # 计算 token 数\n",
    "\n",
    "# 创建 trimmer\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=count_message_tokens,  # 使用自定义 token 计数器\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# 示例对话\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是个好助手\"),\n",
    "    HumanMessage(content=\"你好，我是 bob\"),\n",
    "    AIMessage(content=\"你好！\"),\n",
    "    HumanMessage(content=\"我喜欢香草冰淇淋\"),\n",
    "    AIMessage(content=\"非常好！\"),\n",
    "    HumanMessage(content=\"2 + 2等于几\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"谢谢\"),\n",
    "    AIMessage(content=\"不客气!\"),\n",
    "    HumanMessage(content=\"你开心吗?\"),\n",
    "    AIMessage(content=\"是的!\"),\n",
    "]\n",
    "\n",
    "# 裁剪消息\n",
    "trimmed_messages = trimmer.invoke(messages)\n",
    "print(trimmed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e5522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b17b2f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的名字是 Bob。很高兴认识你！\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"我的名字是什么？\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9c7e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你问了“2 + 2等于几”，这个问题是一个简单的数学问题，答案是4。\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"我问了什么数学问题？\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea9f6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以，Todd！这里有个新的笑话：\n",
      "\n",
      "为什么电脑总是很懒？\n",
      "\n",
      "因为它怕“开机关机”啊！ 😄\n",
      "\n",
      "希望这个笑话让你笑一笑！如果还想听更多，随时告诉我！"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"嗨，我是 Todd，请给我讲个笑话。\"\n",
    "language = \"中文\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
