{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8039b5bc",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0273b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorchvideo需要修改依赖，导航到此路径： .venv\\Lib\\site-packages\\pytorchvideo\\transforms\\augmentations.py\n",
    "# 然后修改第9行代码 \"import torchvision.transforms.functional_tensor as F_t\"  为：\n",
    "# \"import torchvision.transforms._functional_tensor as F_t\"   新增了 _ 符号\n",
    "\n",
    "# 下载视频预训练模型：uv run modelscope download --model tiansz/videomae-base --local_dir models/videomae-base\n",
    "\n",
    "# 训练集链接：https://hf-mirror.com/datasets/sayakpaul/ucf101-subset/blob/main/UCF101_subset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc9f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pytorchvideo.data\n",
    "import torch\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4391a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10  # 模型学习训练集的轮次\n",
    "BATCH_SIZE = 2  # 模型并行学习的样本数量\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = \"../models/videomae-base\"  # 预训练模型路径\n",
    "VIDEO_PATH = \"../datasets/视频分类数据集\"  # 图像训练集路径\n",
    "OUPUT_CHECKPOINT_PATH = \"../models/video_classification_checkpoint\"  # 训练过程中产生的模型文件\n",
    "OUTPUT_MODEL_PATH = \"../models/video_classification_model\"  # 微调后的文本分类模型路径\n",
    "ACCURACY_PATH = \"../common/accuracy.py\"  # 评估脚本本地路径\n",
    "VIDEO_TYPE = \"avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80268e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集视频数量： 20\n",
      "验证集视频数量： 6\n",
      "测试集视频数量： 21\n",
      "视频标签映射关系： {'BaseballPitch': 0, 'Basketball': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset_root_path = pathlib.Path(VIDEO_PATH)\n",
    "\n",
    "train_video_file_paths = list(dataset_root_path.glob(f\"train/*/*.{VIDEO_TYPE}\"))\n",
    "val_video_file_paths = list(dataset_root_path.glob(f\"val/*/*.{VIDEO_TYPE}\"))\n",
    "test_video_file_paths = list(dataset_root_path.glob(f\"test/*/*.{VIDEO_TYPE}\"))\n",
    "print(\"训练集视频数量：\", len(train_video_file_paths))\n",
    "print(\"验证集视频数量：\", len(val_video_file_paths))\n",
    "print(\"测试集视频数量：\", len(test_video_file_paths))\n",
    "# 获取指定目录下所有视频文件的路径\n",
    "all_video_file_paths = (\n",
    "    train_video_file_paths + val_video_file_paths + test_video_file_paths\n",
    ")\n",
    "\n",
    "# 每个类别标签映射到一个唯一的整数标识符\n",
    "cur_sep = os.path.sep\n",
    "class_labels = sorted({str(path).split(cur_sep)[4] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(\"视频标签映射关系：\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4f5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at ../models/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 处理视频数据的图像处理器\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(PRETRAINED_MODEL_NAME_OR_PATH)\n",
    "\n",
    "# 加载预训练模型用于视频分类任务\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME_OR_PATH,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 获取图像的均值和标准差\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "\n",
    "# 确定图像的目标尺寸\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffdd3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 频帧的采样方式、数据增强变换等。并且验证集不进行数据增强，更好地模拟模型在实际使用中的情况\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e05b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb462c42",
   "metadata": {},
   "source": [
    "#### 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9972c5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 11:15, Epoch 9/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.778638</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.547746</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229008</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.257173</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.148985</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725921</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.635504</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.357455</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.812296</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.846175</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['../models/video_classification_model\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    OUPUT_CHECKPOINT_PATH,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    max_steps=(train_dataset.num_videos // BATCH_SIZE) * EPOCHS,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(ACCURACY_PATH)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.half()\n",
    "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "image_processor.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "# 这里轮次显示很大，但没有问题，不必在意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aef93e",
   "metadata": {},
   "source": [
    "#### 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e1e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.54931640625, 'label': 'BaseballPitch'}, {'score': 0.45068359375, 'label': 'Basketball'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "video_cls = pipeline(\n",
    "    task=\"video-classification\",\n",
    "    model=OUTPUT_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\n",
    "    video_cls(\n",
    "        \"../datasets/视频分类数据集/test/BaseballPitch/v_BaseballPitch_g24_c04.avi\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
